defaults:
  - _self_
  - vmas_painting_config

# Defines the sub-task implementation. One of ['nav, 'mix', 'full']
task_type: "full"
debug: False

# Defines the number of steps in each run, and thus the evaluation period.
max_steps: 300
n_agents: 3
n_goals: 6
goals_from_image: null

# Do agents have a multi-head (ie. multi-agent per robot)
# If true include in group map.
multi_head: True

group_map: {
             'nav_agents': [ 'nav-agent_0', 'nav-agent_1', 'nav-agent_2'],
             "speakers": [ 'speak-agent_0', 'speak-agent_1', 'speak-agent_2'],
             "listeners": [ 'listen-agent_0', 'listen-agent_1', 'listen-agent_2']
}
#group_map: {
#             'nav_agents': [ 'nav-agent_0', 'nav-agent_1', 'nav-agent_2'],
#             "mix_agents": [ 'mix-agent_0', 'mix-agent_1', 'mix-agent_2'],
#}


# Whether other agents positions are included in the observation space.
observe_other_agents: False

# Is agent knowledge vector randomly generated, or evenly distributed.
random_knowledge: False
random_source_dim: True
random_all_dims: False

# Do agents need to learn the mixing coefficients to produce their goal.
learn_mix: True
learn_coms: True

# TODO: If we extend to higher dimensionality (>8) use a better metric than L2.
# Maximum euclidean distance from goal to consider payload mix successful.
#  NOTE: values higher than 0.1 result in poor solution matches.
mixing_thresh: 0.1

# Minimum L2 dist between agents to be able to communicate.
coms_proximity: 5
coms_thresh: 0.1

# Fixed, per-step reward when all agents have achieved the sub-tasks.
final_pos_reward: 0.2
final_mix_reward: 0.2
final_coms_reward: 0.2
goal_completion_reward: 0

# Determines if final rewards are given when all agents have achieved task (False)
#  or as n/total for each n agents achieving task (True).
per_agent_final: False

# Shaping on/off and scaling factor
pos_shaping: True
pos_shaping_factor: 1
mix_shaping: True
mix_shaping_factor: 1

# Penalties for agent collisions
agent_collision_penalty: -0.2
env_collision_penalty: -0.2
