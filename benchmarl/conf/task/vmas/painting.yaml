defaults:
  - _self_
  - vmas_painting_config

# Defines the sub-task implementation. One of ['nav, 'mix', 'full']
task_type: "full"
debug: False

# Defines the number of steps in each run, and thus the evaluation period.
max_steps: 300
n_agents: 5
n_goals: 5
goals_from_image: null

# Do agents have a multi-head (ie. multi-agent per robot)
# If true include in group map.
multi_head: True

group_map: {
             'nav_agents': [ 'nav-agent_0', 'nav-agent_1', 'nav-agent_2',
                             'nav-agent_3', 'nav-agent_4'],
             "speakers": [ 'speak-agent_0', 'speak-agent_1', 'speak-agent_2',
                           'speak-agent_3', 'speak-agent_4',],
             "listeners": [ 'listen-agent_0', 'listen-agent_1', 'listen-agent_2',
                            'listen-agent_3', 'listen-agent_4']
}

# Whether other agents positions are included in the observation space.
observe_other_agents: False

# One-hot case: Is agent source dimension randomly assigned.
random_source_dim: True
# Is agent knowledge vector randomly generated, or evenly distributed.
random_knowledge: True
# Is agent knowledge distributed across all knowledge dims
random_all_dims: True

# Do agents need to learn the mixing coefficients to produce their goal.
learn_mix: True
learn_coms: True

# TODO: If we extend to higher dimensionality (>8) use a better metric than L2.
# Maximum euclidean distance from goal to consider payload mix successful.
mixing_thresh: 0.15

# Minimum L2 dist between agents to be able to communicate.
coms_proximity: 5
coms_thresh: 0.1

# Fixed, per-step reward when all agents have achieved the sub-tasks.
final_pos_reward: 0.2
final_mix_reward: 0.2
final_coms_reward: 0.2
goal_completion_reward: 0

# Determines if final rewards are given when all agents have achieved task (False)
#  or as n/total for each n agents achieving task (True).
per_agent_final: False

# Shaping on/off and scaling factor
pos_shaping: True
pos_shaping_factor: 1
mix_shaping: True
mix_shaping_factor: 1

# Penalties for agent collisions
agent_collision_penalty: -0.2
env_collision_penalty: -0.2
